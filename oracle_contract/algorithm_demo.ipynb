{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle Consensus Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_oracles = 20\n",
    "failing_percentage = 0.2\n",
    "N_failing_oracles = round(N_oracles * failing_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x) :\n",
    "    return (np.arctan(x) / np.pi) + 0.5\n",
    "\n",
    "def denormalize(y) :\n",
    "    return np.tan(np.pi * (y - 0.5))\n",
    "\n",
    "def generate_oracles(N_oracles, N_failing_oracles, e=0, sigma=1):\n",
    "    oracles = [\n",
    "        (normalize(np.random.normal(e, sigma)), True)\n",
    "        if i >= N_failing_oracles \n",
    "        else (np.random.uniform(0, 1), False)\n",
    "        for i in range(N_oracles)\n",
    "    ]\n",
    "\n",
    "    np.random.shuffle(oracles)\n",
    "\n",
    "    disjoint = tuple_listes = tuple(map(list, zip(*oracles)))\n",
    "\n",
    "    return disjoint\n",
    "\n",
    "true_essence = 0\n",
    "true_sigma = 1\n",
    "\n",
    "\n",
    "oracles, true_oracles = generate_oracles(N_oracles, N_failing_oracles, true_essence, true_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essence_estimate(oracles : List[Tuple[float, bool]]):\n",
    "    '''\n",
    "        By default, average\n",
    "    '''\n",
    "\n",
    "    weight = 0\n",
    "    result = 0\n",
    "\n",
    "    for value, activated in oracles:\n",
    "        if activated :\n",
    "            weight += 1\n",
    "            result += value\n",
    "    \n",
    "    return result / weight\n",
    "\n",
    "\n",
    "def one_oracle_score(oracle_value, essence_estimate):\n",
    "    '''\n",
    "        Let's say the oracle score is defined by the squared euclidean distance to the essence estimate\n",
    "    '''\n",
    "    return np.linalg.norm(oracle_value - essence_estimate) ** 2\n",
    "\n",
    "\n",
    "def compute_scores(oracles : List[Tuple[float, bool]], only_active : bool = False) :\n",
    "    essence = essence_estimate(oracles)\n",
    "    return [\n",
    "        one_oracle_score(value, essence)\n",
    "        if activated or not only_active else None\n",
    "        for value, activated in oracles\n",
    "    ]\n",
    "\n",
    "def argmin(T):\n",
    "    min_value = None \n",
    "    min_idx = None\n",
    "    for idx, value in enumerate(T):\n",
    "        if value is None : continue\n",
    "\n",
    "        if min_value is None :\n",
    "            min_value = value\n",
    "            min_idx = idx\n",
    "            continue\n",
    "\n",
    "        if value < min_value :\n",
    "            min_value = value\n",
    "            min_idx = idx\n",
    "\n",
    "    return min_idx\n",
    "\n",
    "def find_the_worst_oracle(oracles : List[Tuple[float, bool]]) :\n",
    "    return argmin(compute_scores(oracles, True))\n",
    "\n",
    "def remove_worst_oracles(oracles : List[float], N_failing_oracles : int) :\n",
    "    result = [(oracle, True) for oracle in oracles]\n",
    "\n",
    "    for _ in range(N_failing_oracles):\n",
    "        worst = find_the_worst_oracle(result) \n",
    "        result[worst] = (result[worst][0], False)\n",
    "\n",
    "    return result, compute_scores(result)\n",
    "\n",
    "result, scores = remove_worst_oracles(oracles, N_failing_oracles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- [ ]\t Expected : False \t Found : False \t Score : 0.005539464763822209\n",
      "- [X]\t Expected : True \t Found : False \t Score : 0.002421227690621136\n",
      "- [ ]\t Expected : True \t Found : True \t Score : 0.16815244873041238\n",
      "- [ ]\t Expected : True \t Found : True \t Score : 0.12661957836652465\n",
      "- [ ]\t Expected : True \t Found : True \t Score : 0.008132612158463028\n",
      "- [ ]\t Expected : False \t Found : False \t Score : 0.00041271397805076374\n",
      "- [ ]\t Expected : True \t Found : True \t Score : 0.12583723867331897\n",
      "- [X]\t Expected : False \t Found : True \t Score : 0.012433813633480065\n",
      "- [ ]\t Expected : True \t Found : True \t Score : 0.03807057291944793\n",
      "- [ ]\t Expected : True \t Found : True \t Score : 0.041201764407771\n",
      "- [X]\t Expected : True \t Found : False \t Score : 0.005827147347858098\n",
      "- [ ]\t Expected : True \t Found : True \t Score : 0.03214902479206026\n",
      "- [X]\t Expected : False \t Found : True \t Score : 0.07876921954797243\n",
      "- [ ]\t Expected : True \t Found : True \t Score : 0.07313594362261162\n",
      "- [ ]\t Expected : True \t Found : True \t Score : 0.01344788336430461\n",
      "- [ ]\t Expected : True \t Found : True \t Score : 0.029863248242525352\n",
      "- [ ]\t Expected : True \t Found : True \t Score : 0.015158034064198822\n",
      "- [ ]\t Expected : True \t Found : True \t Score : 0.009207397065777877\n",
      "- [ ]\t Expected : True \t Found : True \t Score : 0.019952127842553857\n",
      "- [ ]\t Expected : True \t Found : True \t Score : 0.016096562577762343\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(result)) :\n",
    "    expected = true_oracles[i]\n",
    "    found = result[i][1]\n",
    "    s = \"[ ]\" if expected == found else \"[X]\"\n",
    "\n",
    "    print(f\"- {s}\\t Expected : {expected} \\t Found : {found} \\t Score : {scores[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5766983183486516 \t 1.633123935319537e+16\n"
     ]
    }
   ],
   "source": [
    "normalized_error = np.abs(essence_estimate(result) - true_essence) \n",
    "denormalized_error = np.abs(denormalize(essence_estimate(result)) - denormalize(true_essence))\n",
    "\n",
    "print(f\"{normalized_error} \\t {denormalized_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method is clearly not effective.\n",
    "Possibles issues :\n",
    "- Current essence estimate would work for a gaussian noise but not for the normalized gaussian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algorithmic-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
